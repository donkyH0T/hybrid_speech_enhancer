import os
import glob
import numpy as np
import librosa
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import random
from tensorflow import keras
import soundfile as sf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import shutil
import time
import warnings
warnings.filterwarnings('ignore')
from concurrent.futures import ThreadPoolExecutor
import multiprocessing

# === –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
SR = 16000
N_FFT = 512
HOP_LENGTH = 256
EPOCHS = 100
BATCH_SIZE = 8
TARGET_TIME_FRAMES = 256
FREQ_BINS = N_FFT // 2 + 1  # 257
# MAX_SAMPLES = 100
MAX_SAMPLES = 1000
CONTEXT_FRAMES = 5

def setup_colab_environment():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Google Colab"""
    print("="*60)
    print("–ù–ê–°–¢–†–û–ô–ö–ê GOOGLE COLAB")
    print("="*60)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"‚úÖ GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {len(gpus)} —É—Å—Ç—Ä–æ–π—Å—Ç–≤")
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("‚úÖ Memory growth –≤–∫–ª—é—á–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ GPU: {e}")
    else:
        print("‚ùå GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    temp_dir = "/tmp/speech_enhancement"
    os.makedirs(temp_dir, exist_ok=True)
    os.makedirs(f"{temp_dir}/data/clean_speech", exist_ok=True)
    os.makedirs(f"{temp_dir}/data/noise", exist_ok=True)
    os.makedirs(f"{temp_dir}/models", exist_ok=True)
    
    print(f"üìÅ –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {temp_dir}")
    print("="*60)
    
    return temp_dir

def copy_data_to_tmp(drive_path, temp_dir):
    """–ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å Google –î–∏—Å–∫–∞ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π SSD"""
    print("\nüìÅ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π SSD...")
    
    drive_data_path = os.path.join(drive_path, "data")
    
    if os.path.exists(drive_data_path):
        # –ö–æ–ø–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–ª–∏ —Ä–∞–Ω–µ–µ
        clean_src = os.path.join(drive_data_path, "clean_speech")
        noise_src = os.path.join(drive_data_path, "noise")
        
        clean_dst = os.path.join(temp_dir, "data/clean_speech")
        noise_dst = os.path.join(temp_dir, "data/noise")
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞ (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å)
        max_files = 10000
        
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        def copy_files(src_dir, dst_dir, file_type="wav"):
            os.makedirs(dst_dir, exist_ok=True)
            files = glob.glob(os.path.join(src_dir, f"*.{file_type}"))[:max_files]
            
            print(f"–ö–æ–ø–∏—Ä—É–µ–º {len(files)} —Ñ–∞–π–ª–æ–≤ –∏–∑ {src_dir}...")
            
            for i, src_file in enumerate(files):
                if i % 100 == 0:
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {i}/{len(files)} —Ñ–∞–π–ª–æ–≤")
                
                dst_file = os.path.join(dst_dir, os.path.basename(src_file))
                if not os.path.exists(dst_file):
                    shutil.copy2(src_file, dst_file)
            
            return len(files)
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
        clean_count = copy_files(clean_src, clean_dst, "wav")
        noise_count = copy_files(noise_src, noise_dst, "wav")
        
        print(f"‚úÖ –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {clean_count} —á–∏—Å—Ç—ã—Ö –∏ {noise_count} —à—É–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        return clean_dst, noise_dst
    else:
        print(f"‚ö†Ô∏è –ü–∞–ø–∫–∞ {drive_data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return None, None

def process_single_file(args):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞ —Å —Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞"""
    clean_path, noisy_path, target_frames = args
    
    try:
        # 1. –†–ê–°–°–ß–ò–¢–´–í–ê–ï–ú —Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–∫—É–Ω–¥ –¥–ª—è –Ω—É–∂–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–∞–¥—Ä–æ–≤
        target_samples = target_frames * HOP_LENGTH  # 256 * 256 = 65536 samples
        target_seconds = target_samples / SR         # 65536 / 16000 = 4.096 —Å–µ–∫—É–Ω–¥
        
        # 2. –ó–ê–ì–†–£–ñ–ê–ï–ú —Å –ü–†–ê–í–ò–õ–¨–ù–û–ô –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
        clean_audio = librosa.load(
            clean_path, 
            sr=SR, 
            duration=target_seconds  # –ó–∞–≥—Ä—É–∂–∞–µ–º –†–û–í–ù–û —Å—Ç–æ–ª—å–∫–æ, —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ
        )[0]
        
        noisy_audio = librosa.load(
            noisy_path,
            sr=SR,
            duration=target_seconds
        )[0]
        
        # 3. –¢–û–ß–ù–û–ï –í–´–†–ê–í–ù–ò–í–ê–ù–ò–ï –î–õ–ò–ù–´
        # –ï—Å–ª–∏ –∞—É–¥–∏–æ –∫–æ—Ä–æ—á–µ –Ω—É–∂–Ω–æ–≥–æ - –¥–æ–ø–æ–ª–Ω—è–µ–º —Ç–∏—à–∏–Ω–æ–π
        if len(clean_audio) < target_samples:
            pad_len = target_samples - len(clean_audio)
            clean_audio = np.pad(clean_audio, (0, pad_len), mode='constant')
        
        # –ï—Å–ª–∏ –¥–ª–∏–Ω–Ω–µ–µ - –æ–±—Ä–µ–∑–∞–µ–º
        clean_audio = clean_audio[:target_samples]
        
        # –¢–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è —à—É–º–Ω–æ–≥–æ
        if len(noisy_audio) < target_samples:
            pad_len = target_samples - len(noisy_audio)
            noisy_audio = np.pad(noisy_audio, (0, pad_len), mode='constant')
        
        noisy_audio = noisy_audio[:target_samples]
        
        # 4. STFT —Å –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ú–ò –ü–ê–†–ê–ú–ï–¢–†–ê–ú–ò
        clean_spec = librosa.stft(
            clean_audio,
            n_fft=N_FFT,
            hop_length=HOP_LENGTH,
            win_length=N_FFT,  # –í–∞–∂–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏!
            window='hann',
            center=False  # –£–±–∏—Ä–∞–µ–º center –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞–¥—Ä–æ–≤
        )
        
        noisy_spec = librosa.stft(
            noisy_audio,
            n_fft=N_FFT,
            hop_length=HOP_LENGTH,
            win_length=N_FFT,
            window='hann',
            center=False
        )
        
        # 5. –ü–†–û–í–ï–†–ö–ê –†–ê–ó–ú–ï–†–ê (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ç–æ—á–Ω–æ target_frames)
        # –ü—Ä–∏ center=False: –∫–∞–¥—Ä–æ–≤ = ‚åà(len(audio) - n_fft) / hop_length‚åâ + 1
        # –ü—Ä–∏ –Ω–∞—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 256 –∫–∞–¥—Ä–æ–≤ —Ç–æ—á–Ω–æ
        
        if clean_spec.shape[1] != target_frames:
            # –õ–æ–≥–∏—Ä—É–µ–º, –Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º
            print(f"‚ö†Ô∏è {os.path.basename(clean_path)}: "
                  f"–æ–∂–∏–¥–∞–ª–∏ {target_frames}, –ø–æ–ª—É—á–∏–ª–∏ {clean_spec.shape[1]} –∫–∞–¥—Ä–æ–≤")
            
            if clean_spec.shape[1] > target_frames:
                clean_spec = clean_spec[:, :target_frames]
                noisy_spec = noisy_spec[:, :target_frames]
            else:
                pad_width = target_frames - clean_spec.shape[1]
                clean_spec = np.pad(clean_spec, ((0, 0), (0, pad_width)), 
                                   mode='constant', constant_values=0)
                noisy_spec = np.pad(noisy_spec, ((0, 0), (0, pad_width)), 
                                   mode='constant', constant_values=0)
        
        # 6. –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï –í 2 –ö–ê–ù–ê–õ–ê (real, imag)
        clean_spec_2ch = np.stack([np.real(clean_spec), np.imag(clean_spec)], axis=-1)
        noisy_spec_2ch = np.stack([np.real(noisy_spec), np.imag(noisy_spec)], axis=-1)
        
        # 7. –í–ï–†–ò–§–ò–ö–ê–¶–ò–Ø
        filename = os.path.basename(clean_path)
        if clean_spec_2ch.shape[1] != target_frames:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏
            print(f"‚ùå {filename}: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ {clean_spec_2ch.shape} "
                  f"(–∫–∞–¥—Ä–æ–≤: {clean_spec_2ch.shape[1]}, –æ–∂–∏–¥–∞–ª–∏ {target_frames})")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
        if np.any(np.isnan(clean_spec_2ch)) or np.any(np.isinf(clean_spec_2ch)):
            print(f"‚ùå {filename}: –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN/Inf –∑–Ω–∞—á–µ–Ω–∏—è")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–ª–Ω—É—é —Ç–∏—à–∏–Ω—É (–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª)
        if np.max(np.abs(clean_spec_2ch)) < 1e-6:
            print(f"‚ö†Ô∏è {filename}: –≤–æ–∑–º–æ–∂–Ω–æ —Ç–∏—Ö–∏–π –∏–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª")
            # –ú–æ–∂–Ω–æ –ª–∏–±–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å, –ª–∏–±–æ –æ—Å—Ç–∞–≤–∏—Ç—å
        
        return noisy_spec_2ch, clean_spec_2ch
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {os.path.basename(clean_path)}: {str(e)[:100]}")
        return None

def prepare_data_parallel(file_tuples, num_samples=1000, target_time_frames=256):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö —è–¥–µ—Ä CPU"""
    print(f"\n‚ö° –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê {min(num_samples, len(file_tuples))} –§–ê–ô–õ–û–í")
    
    start_time = time.time()
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
    file_tuples = file_tuples[:min(num_samples, len(file_tuples))]
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    args_list = [(clean, noisy, target_time_frames) for clean, noisy in file_tuples]
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    noisy_specs = []
    clean_specs = []
    successful = 0
    
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {multiprocessing.cpu_count()} —è–¥–µ—Ä CPU")
    
    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
        # –ú–∞–ø–ø–∏–Ω–≥ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        results = list(executor.map(process_single_file, args_list))
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for result in results:
        if result is not None:
            noisy, clean = result
            noisy_specs.append(noisy)
            clean_specs.append(clean)
            successful += 1
    
    elapsed = time.time() - start_time
    
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {successful}/{len(file_tuples)} —Ñ–∞–π–ª–æ–≤")
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {elapsed:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"üìä –°–∫–æ—Ä–æ—Å—Ç—å: {successful/elapsed:.1f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫")
    
    if successful == 0:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞!")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º—É –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìè –§–û–†–ú–ê –î–ê–ù–ù–´–•:")
    print(f"noisy_specs[0] shape: {noisy_specs[0].shape if noisy_specs else '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}")
    print(f"clean_specs[0] shape: {clean_specs[0].shape if clean_specs else '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}")
    
    return np.array(noisy_specs), np.array(clean_specs)

def build_better_hybrid_model(input_shape, context_frames=5):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ª—É—á—à–∏–º–∏ skip connections"""
    inputs = layers.Input(shape=input_shape)  # (freq, time, channels)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –∫–∞–¥—Ä –≤—Ö–æ–¥–∞
    input_center = layers.Lambda(lambda x: x[:, :, context_frames//2, :])(inputs)
    
    # ========== ENCODER ==========
    # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫
    x = layers.Conv2D(32, (3, 3), padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU(alpha=0.1)(x)
    skip1 = x
    
    # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫
    x = layers.Conv2D(64, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU(alpha=0.1)(x)
    skip2 = x
    
    # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫
    x = layers.Conv2D(128, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU(alpha=0.1)(x)
    
    # ========== BOTTLENECK ==========
    x = layers.Conv2D(128, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU(alpha=0.1)(x)
    
    # ========== DECODER ==========
    # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫ –¥–µ–∫–æ–¥–µ—Ä–∞ + skip connection
    x = layers.Conv2D(64, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU(alpha=0.1)(x)
    x = layers.Add()([x, skip2])  # Skip connection 2
    
    # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫ –¥–µ–∫–æ–¥–µ—Ä–∞ + skip connection
    x = layers.Conv2D(32, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU(alpha=0.1)(x)
    x = layers.Add()([x, skip1])  # Skip connection 1
    
    # ========== OUTPUT ==========
    # –ë–µ—Ä–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –∫–∞–¥—Ä
    x = layers.Lambda(lambda x: x[:, :, context_frames//2, :])(x)
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–π
    x = layers.Conv1D(32, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU(alpha=0.1)(x)
    
    outputs = layers.Conv1D(2, 3, padding='same')(x)
    
    # –ì–õ–û–ë–ê–õ–¨–ù–´–ô SKIP: –≤—ã—Ö–æ–¥ = –≤—Ö–æ–¥ + –∏–∑–º–µ–Ω–µ–Ω–∏—è
    outputs = layers.Add()([input_center, outputs])
    outputs = layers.Activation('tanh')(outputs)
    
    model = models.Model(inputs, outputs)
    
    print(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è hybrid –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    model.summary()
    
    return model

def complex_mse_loss(y_true, y_pred):
    """MSE loss –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —á–∏—Å–µ–ª"""
    return tf.reduce_mean(tf.square(y_true - y_pred))

def create_context_windows(data, context_frames=CONTEXT_FRAMES):
        """–°–æ–∑–¥–∞–µ—Ç –æ–∫–Ω–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."""
        num_samples, freq, time, channels = data.shape
        
        # –°–æ–∑–¥–∞–µ–º —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å–∏
        windows = []
        for i in range(num_samples):
            sample_windows = []
            for t in range(time - context_frames + 1):
                # –ë–µ—Ä–µ–º context_frames –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤
                window = data[i, :, t:t+context_frames, :]
                sample_windows.append(window)
            windows.extend(sample_windows)
        
        return np.array(windows)

def create_center_frames(data, context_frames=CONTEXT_FRAMES):
    """–°–æ–∑–¥–∞–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ –∫–∞–¥—Ä—ã –¥–ª—è Y."""
    num_samples, freq, time, channels = data.shape
    center_idx = context_frames // 2
    result = []
    for i in range(num_samples):
        for t in range(time - context_frames + 1):
            center_frame = data[i, :, t + center_idx, :]
            result.append(center_frame)
    return np.array(result)

# === –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ ===
if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è Colab
    TEMP_DIR = setup_colab_environment()
    
    # –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É –Ω–∞ Google –î–∏—Å–∫–µ (–∏–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–π)
    DRIVE_PROJECT_PATH = "/content/drive/MyDrive/diplom-project"
    
    # 1. –ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π SSD
    clean_dir, noise_dir = copy_data_to_tmp(DRIVE_PROJECT_PATH, TEMP_DIR)
    
    if clean_dir is None or noise_dir is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ. –í—ã—Ö–æ–¥.")
        exit(1)
    
    # 2. –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
    clean_files = glob.glob(os.path.join(clean_dir, "*.wav"))
    print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ {len(clean_files)} —á–∏—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã —Ñ–∞–π–ª–æ–≤ (—á–∏—Å—Ç—ã–π, —à—É–º–Ω—ã–π)
    file_tuples = []
    for clean_file in clean_files:
        noisy_file = os.path.join(noise_dir, os.path.basename(clean_file))
        if os.path.exists(noisy_file):
            file_tuples.append((clean_file, noisy_file))
        else:
            print(f"‚ö†Ô∏è –®—É–º–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {noisy_file}")
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(file_tuples)} –ø–∞—Ä —Ñ–∞–π–ª–æ–≤")
    
    if len(file_tuples) == 0:
        print("‚ùå –ù–µ—Ç –ø–∞—Ä —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –í—ã—Ö–æ–¥.")
        exit(1)
        
    # 4. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
    if len(file_tuples) > MAX_SAMPLES:
        print(f"üîß –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {MAX_SAMPLES} –ø–∞—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞")
        file_tuples = file_tuples[:MAX_SAMPLES]
    
    # 5. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüéØ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
    X_all, Y_all = prepare_data_parallel(
        file_tuples,
        num_samples=len(file_tuples),
        target_time_frames=TARGET_TIME_FRAMES
    )
    val_split = 0.15
    split_idx = int(len(X_all) * (1 - val_split))
    
    X_train, X_val = X_all[:split_idx], X_all[split_idx:]
    Y_train, Y_val = Y_all[:split_idx], Y_all[split_idx:]

    X_train_context = create_context_windows(X_train, CONTEXT_FRAMES)
    X_val_context = create_context_windows(X_val, CONTEXT_FRAMES)
    Y_train_center = create_center_frames(Y_train, CONTEXT_FRAMES)
    Y_val_center = create_center_frames(Y_val, CONTEXT_FRAMES)
    print(f"\n‚úÖ –î–ê–ù–ù–´–ï –ü–ï–†–ï–§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–´:")
    print(f"X_train_context: {X_train_context.shape} (–æ–∫–Ω–∞ —Ö freq —Ö context —Ö channels)")
    print(f"Y_train_center:  {Y_train_center.shape} (–æ–∫–Ω–∞ —Ö freq —Ö channels)")
    print(f"X_val_context:   {X_val_context.shape}")
    print(f"Y_val_center:    {Y_val_center.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
    if len(X_train_context) != len(Y_train_center):
        print(f"‚ùå –û–®–ò–ë–ö–ê: –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤!")
        print(f"X_train_context: {len(X_train_context)} –æ–∫–æ–Ω")
        print(f"Y_train_center:  {len(Y_train_center)} –æ–∫–æ–Ω")
        exit(1)
        
    input_shape = (FREQ_BINS, CONTEXT_FRAMES, 2)  
    # 9. –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    print("\nü§ñ –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
    model = build_better_hybrid_model(input_shape)
    
    model.compile(
        optimizer=optimizers.Adam(learning_rate=0.001),
        loss=complex_mse_loss,
        metrics=['mae']
    )
    
    model.summary()
    
    # 11. Callbacks
    model_dir = os.path.join(TEMP_DIR, "models")
    os.makedirs(model_dir, exist_ok=True)
    
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-6,
            verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            os.path.join(model_dir, "best_model.weights.h5"),
            monitor='val_loss',
            save_best_only=True,
            save_weights_only=True,
            verbose=1
        )
    ]
    
    # 12. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å (—Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤—ã–µ 2 —ç–ø–æ—Ö–∏)
    print("\nüöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*60)
    
    history = model.fit(
        X_train_context, Y_train_center,
        batch_size=min(BATCH_SIZE, len(X_train_context)),
        epochs=EPOCHS,
        validation_data=(X_val_context, Y_val_center),
        shuffle=True,
        verbose=1,
        callbacks=callbacks
    )
    
    # 13. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    final_model_path = os.path.join(model_dir, "final_model.h5")
    model.save(final_model_path)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}")
    
    # 14. –ö–æ–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ Google –î–∏—Å–∫
    print("\nüíæ –ö–û–ü–ò–†–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ù–ê GOOGLE –î–ò–°–ö...")
    drive_models_dir = os.path.join(DRIVE_PROJECT_PATH, "models")
    os.makedirs(drive_models_dir, exist_ok=True)
    
    # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏ –ª–æ–≥–∏
    for file in glob.glob(os.path.join(model_dir, "*")):
        shutil.copy2(file, drive_models_dir)
    
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {drive_models_dir}")
    
    print("\n" + "="*60)
    print("üéâ –¢–ï–°–¢–û–í–û–ï –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"–ï—Å–ª–∏ –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç, —É–≤–µ–ª–∏—á—å—Ç–µ:")
    print(f"  - MAX_SAMPLES –¥–æ 1000-2000")
    print(f"  - TEST_EPOCHS –¥–æ {EPOCHS}")
    print("="*60)